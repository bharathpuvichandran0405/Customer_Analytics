# -*- coding: utf-8 -*-
"""Customer_Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103oxLA3VtvYAiM2svmYoTwNimVJhnUJw
"""

import pandas as pd

# Load the dataset
dataset = "/content/online_retail_II.xlsx"
df = pd.read_excel(dataset, sheet_name=0)

df.head()

print(df.isnull().sum())

# Drop the rows where Customer ID is missing
df_cleaned = df.dropna(subset=['Customer ID'])

# Fill missing descriptions with a placeholder
df_cleaned['Description'] = df_cleaned['Description'].fillna('Unknown')

# Convert Customer ID to integer type
df_cleaned['Customer ID'] = df_cleaned['Customer ID'].astype(int)

# Filter out rows with non-positive quantity
df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]

print(df_cleaned.info())  # Check data types

#Check again if the missing values is cleared
print(df_cleaned.isnull().sum())
df_cleaned.head()

import datetime as dt

# Define the snapshot date as the last available transaction date + 1 day
snapshot_date = df_cleaned['InvoiceDate'].max() + dt.timedelta(days=1)
print("Snapshot Date:", snapshot_date)

# Compute RFM metrics
rfm = df_cleaned.groupby('Customer ID').agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency
    'Invoice': 'count',  # Frequency (number of transactions)
    'Price': lambda x: (x * df_cleaned.loc[x.index, 'Quantity']).sum()  # Monetary (total spend)
}).rename(columns={'InvoiceDate': 'Recency', 'Invoice': 'Frequency', 'Price': 'Monetary'})

print(rfm.head())

# Assign scores using quantiles (higher is better for Frequency & Monetary, lower is better for Recency)
rfm['R_Score'] = pd.qcut(rfm['Recency'], q=4, labels=[4, 3, 2, 1])  # Lower recency is better
rfm['F_Score'] = pd.qcut(rfm['Frequency'], q=4, labels=[1, 2, 3, 4])  # Higher frequency is better
rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=4, labels=[1, 2, 3, 4])  # Higher monetary value is better

# Combine RFM scores into a single column
rfm['RFM_Score'] = rfm[['R_Score', 'F_Score', 'M_Score']].astype(int).sum(axis=1)

print(rfm.head())

# Define customer segments based on RFM Score
def segment_customer(score):
    if score >= 10:
        return 'High Value Customers'
    elif score >= 6:
        return 'Medium Value Customers'
    else:
        return 'Low Value Customers'

rfm['Segment'] = rfm['RFM_Score'].apply(segment_customer)

print(rfm.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Set the theme
sns.set(style="whitegrid")

# Plot
plt.figure(figsize=(10, 8))
sns.countplot(x='Segment', data=rfm, order=['High Value Customers', 'Medium Value Customers', 'Low Value Customers'])
plt.title('Count of number of Customers that fall in each segment')
plt.xlabel('Customer Segments')
plt.ylabel('Number of Customers')
plt.show()

segment_counts = rfm['Segment'].value_counts()

# Pie chart
plt.figure(figsize=(6, 6))
plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set2'))
plt.title('Customer Segments Proportion')
plt.axis('equal')
plt.show()

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Step 1: Prepare Features and Target
X = rfm[['Recency', 'Frequency']].values
y = rfm['Monetary'].values

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for CNN: (samples, timesteps, features)
X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)

# Step 2: Build CNN Model
model_CNN = Sequential()
model_CNN.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))
model_CNN.add(MaxPooling1D(pool_size=1))
model_CNN.add(Flatten())
model_CNN.add(Dense(32, activation='relu'))
model_CNN.add(Dense(1))  # Output layer for regression

# Step 3: Compile and Train
model_CNN.compile(optimizer='adam', loss='mse', metrics=['mae'])
model_CNN.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=16, verbose=1)

# Step 4: Evaluate
loss, mae = model_CNN.evaluate(X_test, y_test)
print(f"\n CNN Model Evaluation:\nMAE: {mae:.2f}")

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Predict on test set
y_pred = model_CNN.predict(X_test).flatten()

# RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f" RMSE: {rmse:.2f}")

import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

df = df_cleaned.copy()
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
df['Month'] = df['InvoiceDate'].dt.to_period("M")
snapshot_date = df['InvoiceDate'].max()

# Group by Customer ID and Month to get Recency and Frequency

monthly = df.groupby(['Customer ID', 'Month']).agg({
    'InvoiceDate': 'max',
    'Invoice': 'nunique'
}).reset_index()

monthly['Recency'] = (snapshot_date - monthly['InvoiceDate']).dt.days
monthly.rename(columns={'Invoice': 'Frequency'}, inplace=True)

sequences = monthly.groupby('Customer ID').apply(
    lambda x: x[['Recency', 'Frequency']].values.tolist()
)

# Filter customers with at least 4 months of history
sequences = sequences[sequences.apply(len) >= 4]

# Use only the last 4 months
X = np.array(sequences.apply(lambda x: x[-4:]).tolist())

#Target variable (y) = Total monetary value per customer
df['Amount'] = df['Quantity'] * df['Price']
clv = df.groupby('Customer ID')['Amount'].sum()

# Get CLV only for selected customers
y = clv.loc[sequences.index].values

print("X shape:", X.shape)   # (samples, time_steps, features)
print("y shape:", y.shape)

# Build LSTM model
model_LSTM = Sequential()
model_LSTM.add(LSTM(32, activation='relu', input_shape=(X.shape[1], X.shape[2])))
model_LSTM.add(Dense(16, activation='relu'))
model_LSTM.add(Dense(1))  # Output: Predicted CLV

model_LSTM.compile(optimizer='adam', loss='mse', metrics=['mae'])
history = model_LSTM.fit(X, y, epochs=100, batch_size=8, verbose=1)

y_pred = model_LSTM.predict(X).flatten()
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print("\n LSTM Model Evaluation:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.4f}")

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Add
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Sequence: (num_customers, 4 time steps, 2 features [Recency, Frequency])

X = np.array([
    [[15, 2], [13, 3], [11, 5], [9, 6]],
    [[10, 1], [8, 2], [7, 3], [5, 4]],
    [[20, 1], [19, 1], [17, 2], [16, 3]],
    [[12, 4], [11, 5], [9, 6], [7, 7]],
    [[25, 1], [23, 1], [22, 2], [20, 3]],
])
y = np.array([2000, 2500, 1800, 2700, 1600])


X = np.array([
    [[15, 2], [13, 3], [11, 5], [9, 6]],
    [[10, 1], [8, 2], [7, 3], [5, 4]],
    [[20, 1], [19, 1], [17, 2], [16, 3]],
    [[12, 4], [11, 5], [9, 6], [7, 7]],
    [[25, 1], [23, 1], [22, 2], [20, 3]],
])
y = np.array([2000, 2500, 1800, 2700, 1600])

def transformer_block(inputs, num_heads=2, ff_dim=64, dropout=0.1):
    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)
    attn_output = Dropout(dropout)(attn_output)
    out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attn_output]))

    ffn = Dense(ff_dim, activation='relu')(out1)
    ffn = Dense(inputs.shape[-1])(ffn)
    ffn_output = Dropout(dropout)(ffn)
    return LayerNormalization(epsilon=1e-6)(Add()([out1, ffn_output]))

from tensorflow.keras.layers import Layer

class PositionalEncoding(Layer):
    def call(self, x):
        # x.shape = (batch_size, time_steps, features)
        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)
        positions = tf.cast(positions, tf.float32)
        positions = tf.expand_dims(positions, axis=0)
        positions = tf.expand_dims(positions, axis=-1)
        return x + positions

def build_transformer_model(input_shape):
    inputs = Input(shape=input_shape)
    x = PositionalEncoding()(inputs)
    x = transformer_block(x, num_heads=2, ff_dim=64)
    x = transformer_block(x, num_heads=2, ff_dim=64)
    x = GlobalAveragePooling1D()(x)
    x = Dense(32, activation='relu')(x)
    x = Dense(1)(x)
    model = Model(inputs, outputs=x)
    return model

transformer_model = build_transformer_model(input_shape=(X.shape[1], X.shape[2]))
transformer_model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = transformer_model.fit(X, y, epochs=100, batch_size=2, verbose=1)

y_pred = transformer_model.predict(X).flatten()
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print("\n Transformer Model Evaluation:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.4f}")


import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import numpy as np

# Select features and target
X = rfm[['Recency', 'Frequency']]
y = rfm['Monetary']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create the model
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',  # for regression
    n_estimators=100,              # number of trees
    learning_rate=0.1,             # shrinkage rate
    max_depth=4,                   # depth of each tree
    subsample=0.8,                 # % of data used per tree
    colsample_bytree=0.8,          # % of features used per tree
    random_state=42
)

# Fit the model
xgb_model.fit(X_train, y_train)

# Predict on test set
y_pred = xgb_model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# Print results
print(" XGBoost Model Evaluation")
print(f"MAE (Mean Absolute Error): {mae:.2f}")
print(f"RMSE (Root Mean Squared Error): {rmse:.2f}")
print(f"R² Score: {r2:.4f}")


import pandas as pd
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Feature selection
X = rfm[['Recency', 'Frequency']]
y = rfm['Monetary']

# Split data into train and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

lgb_model = LGBMRegressor(
    objective='regression',
    n_estimators=100,       # Number of trees
    learning_rate=0.1,      # How fast it learns (try 0.01 to 0.3)
    max_depth=4,            # Depth of each tree
    random_state=42         # For reproducibility
)

# Fit model on training data
lgb_model.fit(X_train, y_train)

# Predict on test set
y_pred = lgb_model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(" LightGBM Model Evaluation:")
print(f"MAE (Mean Absolute Error): {mae:.2f}")
print(f"RMSE (Root Mean Squared Error): {rmse:.2f}")
print(f"R² Score: {r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt

# Model Names
models = ['CNN', 'LSTM', 'XGBoost', 'LightGBM', 'Transformer']

# Evaluation Metrics
mae_scores = [1676.77, 3272.77, 1883.86, 1804.11, 2100.77]
rmse_scores = [6847.87, 11128.13, 8023.83, 8371.21, 2141.69]
r2_scores = [0.5454, 0.5046, 0.3758, 0.3206, -25.4219]

# Bar position logic
x = np.arange(len(models))  # model positions
width = 0.25                # width of each bar group

# Plot setup
plt.figure(figsize=(12, 6))

# Plot each metric group
plt.bar(x - width, mae_scores, width, label='MAE', color='red')
plt.bar(x, rmse_scores, width, label='RMSE', color='black')
plt.bar(x + width, r2_scores, width, label='R² Score', color='green')

# Labels and style
plt.ylabel('Score')
plt.title('Model Performance Comparison (MAE, RMSE, R² Score)')
plt.xticks(x, models)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Use rfm.index as customer IDs for prediction alignment
customer_ids = rfm.index.to_numpy()

#  Predict CLV using the trained CNN model
y_pred_cnn = model_CNN.predict(X).flatten()

#  Create DataFrame with predictions
predicted_clv_df = pd.DataFrame({
    'Customer ID': customer_ids,
    'Predicted_CLV': y_pred_cnn
})

#  Sort customers by predicted CLV
predicted_clv_df_sorted = predicted_clv_df.sort_values(by='Predicted_CLV', ascending=False).reset_index(drop=True)
pd.set_option('display.float_format', '{:.2f}'.format)

# Show top results
print(predicted_clv_df_sorted)

# Step 1: Calculate Q1 and Q3
q1 = predicted_clv_df_sorted['Predicted_CLV'].quantile(0.25)
q3 = predicted_clv_df_sorted['Predicted_CLV'].quantile(0.75)

# Step 2: Define segmentation function
def segment_customer(clv, q1, q3):
    if clv >= q3:
        return 'High Value'
    elif clv >= q1:
        return 'Medium Value'
    else:
        return 'Low Value'

# Step 3: Apply segmentation
predicted_clv_df_sorted['CLV_Segment'] = predicted_clv_df_sorted['Predicted_CLV'].apply(segment_customer, args=(q1, q3))

# Step 4: Check top rows
print(predicted_clv_df_sorted.head())

# Step 5: Count customers by segment
segment_counts = predicted_clv_df_sorted['CLV_Segment'].value_counts()

# Step 6: Show segment counts
print("\nCustomer counts by CLV Segment:")
print(segment_counts)

import matplotlib.pyplot as plt

# 1. Pie Chart - Customer Segments
segment_counts = predicted_clv_df_sorted['CLV_Segment'].value_counts()

plt.figure(figsize=(7, 7))
plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=140, colors=['red', 'purple', 'yellow'])
plt.title('Customer Segments by Predicted CLV')
plt.axis('equal')
plt.show()

# Calculate Average Predicted CLV per Segment
avg_clv_per_segment = predicted_clv_df_sorted.groupby('CLV_Segment')['Predicted_CLV'].mean().sort_values(ascending=False)

# Display the result
print("\nAverage Predicted CLV per Customer Segment:")
print(avg_clv_per_segment)

# Calculate Average Predicted CLV per Segment
avg_clv_per_segment = predicted_clv_df_sorted.groupby('CLV_Segment')['Predicted_CLV'].mean().sort_values(ascending=False)

# Display the result
print("\nAverage Predicted CLV per Customer Segment:")
print(avg_clv_per_segment)

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

# Plot Average CLV per Segment
plt.figure(figsize=(8, 5))
avg_clv_per_segment.plot(kind='bar', color=['red', 'black', 'yellow'])

# Title and labels
plt.title('Average Predicted CLV per Customer Segment')
plt.xlabel('CLV Segment')
plt.ylabel('Average CLV')
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}'))
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Define churn label: If Recency > 90 days, churn = 1
rfm['Churn'] = rfm['Recency'].apply(lambda x: 1 if x > 90 else 0)

# Now prepare features and correct target
X = rfm[['Recency', 'Frequency', 'Monetary']]  # Features
y = rfm['Churn']  # Target = Churn (0 or 1)

# Check if churn labels are correct
print(y.value_counts())  # Should show 0 and 1

# Prepare features and target
X = rfm[['Recency', 'Frequency', 'Monetary']]
y = rfm['Churn']

# Train-Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check
print("Training samples:", len(X_train))
print("Testing samples:", len(X_test))

# Import
from xgboost import XGBClassifier

# Build the model
xgb_classifier = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    n_estimators=100,
    learning_rate=0.1,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Train the model
xgb_classifier.fit(X_train, y_train)

# Predict on test set
y_pred = xgb_classifier.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\n Churn Prediction Model Evaluation:")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")

# Show Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Predict churn for all customers (full data, not just test set)
full_churn_pred = xgb_classifier.predict(X)

# Create a new DataFrame
churn_result_df = pd.DataFrame({
    'Customer ID': rfm.index,
    'Predicted Churn': full_churn_pred
})

# Show top results
print(churn_result_df.head())

# How many churned vs not churned
print("\nPredicted Churn Distribution:")
print(churn_result_df['Predicted Churn'].value_counts())

import matplotlib.pyplot as plt

# Churn counts
churn_counts = churn_result_df['Predicted Churn'].value_counts()

# Pie Chart
plt.figure(figsize=(6,6))
plt.pie(churn_counts, labels=['Active (0)', 'Churned (1)'], autopct='%1.1f%%', startangle=140, colors=['red', 'purple'])
plt.title('Customer Churn Prediction Distribution (Pie Chart)')
plt.show()

# Bar Chart
plt.figure(figsize=(6,4))
plt.bar(['Active', 'Churned'], churn_counts.values, color=['red', 'yellow'])
plt.title('Customer Churn Prediction Distribution (Bar Chart)')
plt.ylabel('Number of Customers')
plt.show()

# Only reset churn_result_df (NOT predicted_clv_df_sorted)
churn_result_df = churn_result_df.reset_index(drop=True)

# Merge on 'Customer ID'
combined_df = pd.merge(predicted_clv_df_sorted, churn_result_df, on='Customer ID', how='inner')

# Check if merge is successful
print(combined_df[['Customer ID', 'CLV_Segment', 'Predicted Churn']].head())

# Define Risk Category Function
def assign_risk(row):
    if row['Predicted Churn'] == 1:
        if row['CLV_Segment'] == 'High Value':
            return ' High Value - High Risk'
        elif row['CLV_Segment'] == 'Medium Value':
            return ' Medium Value - At Risk'
        else:
            return ' Low Value - At Risk'
    else:
        if row['CLV_Segment'] == 'High Value':
            return ' High Value - Safe'
        elif row['CLV_Segment'] == 'Medium Value':
            return ' Medium Value - Safe'
        else:
            return ' Low Value - Safe'

# Apply the risk function
combined_df['Risk_Category'] = combined_df.apply(assign_risk, axis=1)

# Check first few rows
print(combined_df[['Customer ID', 'CLV_Segment', 'Predicted Churn', 'Risk_Category']].head())

# How many customers in each risk category?
risk_summary = combined_df['Risk_Category'].value_counts()

print("\nCustomer Count by Risk Category:")
print(risk_summary)

import matplotlib.pyplot as plt

# Pie chart
plt.figure(figsize=(8,8))
risk_summary.plot(kind='pie', autopct='%1.1f%%', startangle=140)
plt.title('Customer Risk Categories Based on CLV and Churn Prediction')
plt.ylabel('')  # Hide y-label
plt.show()

# Step 1: Merge CLV and Churn data
revenue_df = pd.merge(predicted_clv_df_sorted, churn_result_df, on='Customer ID', how='inner')

# Step 2: Filter churned customers
churned_customers = revenue_df[revenue_df['Predicted Churn'] == 1]

# Step 3: Calculate total revenue loss
total_revenue_loss = churned_customers['Predicted_CLV'].sum()

# Display results
print(f"Number of customers predicted to churn: {len(churned_customers)}")
print(f" Estimated Revenue Loss if churn is not prevented: ${total_revenue_loss:,.2f}")

# STEP 2: Create a customer-product matrix using product names (Description)
customer_product_matrix = df_cleaned.pivot_table(
    index='Customer ID',
    columns='Description',
    values='Quantity',
    aggfunc='sum',
    fill_value=0
)

from sklearn.metrics.pairwise import cosine_similarity

item_similarity = cosine_similarity(customer_product_matrix.T)
item_similarity_df = pd.DataFrame(
    item_similarity,
    index=customer_product_matrix.columns,
    columns=customer_product_matrix.columns
)

def recommend_items_verbose(customer_id, top_n=10):
    if customer_id not in customer_product_matrix.index:
        print(f"Customer ID {customer_id} not found.")
        return

    user_vector = customer_product_matrix.loc[customer_id]
    purchased_items = user_vector[user_vector > 0].index.tolist()

    for item in purchased_items:
        print(f"\nBecause customer bought: '{item}'")
        # Get similar items (excluding already purchased ones)
        similar_items = item_similarity_df[item].drop(labels=purchased_items, errors='ignore')
        top_similar = similar_items.sort_values(ascending=False).head(top_n).index.tolist()
        print("Recommend (similar products):")
        for rec in top_similar:
            print(" -", rec)

sample_customer_id = df_cleaned['Customer ID'].iloc[0]
recommend_items_verbose(sample_customer_id)

from collections import Counter
import matplotlib.pyplot as plt

# Create a dictionary to collect all recommended products
recommended_products = Counter()

# Modified function to collect recommended products for multiple customers
def collect_recommendations(customer_ids, top_n=10):
    for customer_id in customer_ids:
        if customer_id not in customer_product_matrix.index:
            continue
        user_vector = customer_product_matrix.loc[customer_id]
        purchased_items = user_vector[user_vector > 0].index.tolist()

        for item in purchased_items:
            similar_items = item_similarity_df[item].drop(labels=purchased_items, errors='ignore')
            top_similar = similar_items.sort_values(ascending=False).head(top_n).index.tolist()
            recommended_products.update(top_similar)

# Pick first 100 customers to avoid long processing
sample_customer_ids = df_cleaned['Customer ID'].unique()[:100]
collect_recommendations(sample_customer_ids, top_n=5)

# Get top 10 most recommended items
top_items = dict(recommended_products.most_common(10))

# Plot
plt.figure(figsize=(10, 6))
plt.barh(list(top_items.keys())[::-1], list(top_items.values())[::-1], color='red', edgecolor='black')
plt.title("Top 10 Recommended Products")
plt.xlabel("Recommendation Frequency")
plt.tight_layout()
plt.show()

# Top 10 At-Risk Customers based on churn probability
top_risk_customers = churn_result_df.sort_values(by='Predicted Churn', ascending=False).head(10)

plt.figure(figsize=(10, 6))
plt.barh(top_risk_customers['Customer ID'].astype(str), top_risk_customers['Predicted Churn'], color='orange', edgecolor='black')
plt.xlabel("Churn Probability")
plt.title("Top 10 At-Risk Customers")
plt.tight_layout()
plt.gca().invert_yaxis()
plt.show()

# Merge churn results with CLV segments
merged_df = churn_result_df.merge(rfm[['Segment']], left_on='Customer ID', right_index=True)

# Cross-tabulate churn per segment
churn_by_segment = pd.crosstab(merged_df['Segment'], merged_df['Predicted Churn'])

# Stacked bar plot
churn_by_segment.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='coolwarm', edgecolor='black')
plt.title("Churn Distribution by CLV Segment")
plt.xlabel("Customer Segment")
plt.ylabel("Number of Customers")
plt.legend(["Active (0)", "Churned (1)"])
plt.tight_layout()
plt.show()

import seaborn as sns

# Compute correlation matrix of top 20 frequently bought items
top_products = df_cleaned['Description'].value_counts().head(20).index
co_purchase_matrix = customer_product_matrix[top_products].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(co_purchase_matrix, annot=True, cmap='YlOrBr', linewidths=0.5)
plt.title("Co-Purchased Product Correlation Heatmap")
plt.tight_layout()
plt.show()

# Count total quantity sold per product
top_selling = df_cleaned.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)

plt.figure(figsize=(10, 6))
plt.barh(top_selling.index[::-1], top_selling.values[::-1], color='skyblue', edgecolor='black')
plt.title("Top 10 Best-Selling Products")
plt.xlabel("Total Quantity Sold")
plt.tight_layout()
plt.show()

df_cleaned.to_csv("df_cleaned.csv", index=False)

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt

# ─── 1) Load your cleaned sample data from GitHub raw (no large CSV upload!) ───
@st.cache_data
def load_data():
    url = (
        "https://raw.githubusercontent.com/"
        "bharathpuvichandran0405/Customer_Analytics/main/df_cleaned_sample.csv"
    )
    return pd.read_csv(url)

df = load_data()

# ─── 2) Streamlit page setup ──────────────────────────────────────────────────
st.set_page_config(page_title="Customer Analytics Dashboard", layout="wide")
st.sidebar.title("📊 Customer Analytics Project")
page = st.sidebar.radio("Go to", ["Customer Lifetime Value", "Churn Prediction", "Product Recommendation"])

# ─── 3) Customer Lifetime Value ───────────────────────────────────────────────
if page == "Customer Lifetime Value":
    st.title("📈 Customer Lifetime Value Prediction")

    if 'Segment' in df.columns:
        segment_counts = df['Segment'].value_counts().reindex(
            ['High Value Customers','Medium Value Customers','Low Value Customers']
        )
        fig, ax = plt.subplots(figsize=(7,5))
        segment_counts.plot(kind='bar', edgecolor='black', ax=ax)
        ax.set_title("Customer Count per CLV Segment")
        ax.set_xlabel("Segment")
        ax.set_ylabel("Number of Customers")
        ax.grid(axis='y', linestyle='--', alpha=0.7)
        st.pyplot(fig)
    else:
        st.warning("No 'Segment' column found in your data.")

# ─── 4) Churn Prediction ───────────────────────────────────────────────────────
elif page == "Churn Prediction":
    st.title("⚠️ Churn Prediction")

    if 'Predicted Churn' in df.columns:
        churn_counts = df['Predicted Churn'].value_counts().sort_index()
        # Pie chart
        fig1, ax1 = plt.subplots(figsize=(5,5))
        ax1.pie(churn_counts, labels=['Active (0)','Churned (1)'], autopct='%1.1f%%', startangle=140)
        ax1.set_title("Churn Prediction Distribution (Pie)")
        st.pyplot(fig1)

        # Bar chart
        fig2, ax2 = plt.subplots(figsize=(6,4))
        churn_counts.plot(kind='bar', color=['green','red'], edgecolor='black', ax=ax2)
        ax2.set_title("Churn Prediction Distribution (Bar)")
        ax2.set_xlabel("Churn Label")
        ax2.set_ylabel("Number of Customers")
        st.pyplot(fig2)
    else:
        st.warning("No 'Predicted Churn' column found in your data.")

# ─── 5) Product Recommendation ─────────────────────────────────────────────────
else:
    st.title("🎯 Product Recommendation System")

    if 'Customer ID' not in df.columns:
        st.error("No 'Customer ID' column found in your data.")
    else:
        customer_ids = sorted(df['Customer ID'].unique())
        selected_id = st.selectbox("Select a Customer ID", customer_ids)

        if st.button("Get Recommendations"):
            # here you’d call your real recommendation logic
            # for demo purposes we’ll show the first 5 of your `recommend_items_verbose` output
            from collections import Counter
            # assume you already built `customer_product_matrix` & `item_similarity_df` above
            # for brevity this snippet just outputs a placeholder:
            recs = [
                "WHITE BEADED GARLAND STRING 20LIGHT",
                "EASTER DECORATION NATURAL CHICK",
                "SKY BLUE COLOUR GLASS GEMS IN BAG",
                "CLASSIC DIAMANTE EARRINGS JET",
                "BAKING MOULD TOFFEE CUP CHOCOLATE"
            ]
            st.success(f"Top 5 recommendations for Customer {selected_id}:")
            for r in recs:
                st.write("✔️", r)





